A Key Observation

When the Gini of Logistic Regression (LR) is already high,
➤ Switching to XGBoost yields limited improvement

But when LR Gini is low,
➤ XGBoost often brings significant gains


🟦 [Insert graph: X-axis = LR Gini, Y-axis = XGB Gini – LR Gini]

Why?

If data is linearly separable, LR works well

When data has complex non-linear structure, LR falls short
➤ Tree-based models like XGBoost handle it better


So the key question becomes:

> Can we estimate the “complexity” of a dataset in advance, to predict whether it’s worth switching from LR to XGBoost?




---

Challenges in Measuring Data Complexity

Metrics like mutual information or covariance only capture relationships between individual features and the label
➤ They ignore interactions between features

Capturing higher-order interactions requires explicit feature crosses, which are computationally expensive


Simple and practical solution:

> ✅ Train a quick LR model on a small data subset
→ Use its Gini as a proxy for data complexity
