2.2 Experimental Setup
This section describes how we generated the empirical data that each sub-framework will learn to predict. Figure 1 (next page) provides a high-level workflow; the narrative below details sampling rules, model training settings, and the rationale for each design choice.

2.2.1 Data Collection & Sampling Strategy
All seven business datasets (see § 2.1) reside on local storage, so “collection” refers solely to drawing appropriately sized, stratified random samples for the experiments. Two principles guided the sampling:

Computational feasibility – each experiment had to finish within one overnight batch window on a 48-core server.

Representativeness – prior A/B checks showed that XGBoost (XGB) trained on 5 million (5 MM) rows yields a Gini within ±0.5 pp of training on the full dataset; therefore 5 MM rows are considered “statistically sufficient.”

Unless otherwise noted, the class distribution of the sample mirrors the original event rate of each dataset.

2.2.2 Algorithm-Impact Experiments (ALG)
Objective. Measure the incremental performance gain from upgrading the modelling algorithm.

Procedure (per dataset, 
𝑛
=
7
n=7).

Draw a 5 MM stratified sample.

Train a baseline Logistic Regression and a benchmark XGB using identical feature sets and five-fold CV.

Record

Δ
𝐺
𝑖
𝑛
𝑖
ALG
=
𝐺
𝑖
𝑛
𝑖
XGB
−
𝐺
𝑖
𝑛
𝑖
LR
ΔGini 
ALG
​
 =Gini 
XGB
​
 −Gini 
LR
​
 
This difference is the target variable that the ALG sub-framework will later predict.

Why 5 MM? It balances run-time (≈ 30 minutes per model) with accuracy and mirrors full-data behaviour, making results transferable to production scale.

Output: 7 data points 
(
Δ
𝐺
𝑖
𝑛
𝑖
ALG
,
  
dataset characteristics
)
(ΔGini 
ALG
​
 ,dataset characteristics).

2.2.3 Data-Size-Impact Experiments (SIZE)
Objective. Quantify how model performance grows with additional data.

Procedure (per dataset).

Generate seven nested training sets with sizes expressed as fractions of 5 MM rows:

{
0.01
,
  
0.05
,
  
0.10
,
  
0.50
,
  
1
,
  
2
,
  
5
}
 MM
{0.01,0.05,0.10,0.50,1,2,5} MM
Train an XGB model on each subset; record the resulting Gini scores.

Fit a performance curve 
𝐺
𝑖
𝑛
𝑖
=
𝑓
(
data size
)
Gini=f(data size). Section 3.1 details the functional form (power-law with three parameters).

Each dataset yields one curve; thus the SIZE sub-framework receives 7 fitted curves, one per dataset.

2.2.4 Feature-Impact Experiments (FEAT)
Objective. Estimate the marginal effect of adding or removing a single feature.

Procedure (per dataset).

From model documentation, take the top-10 and bottom-10 SHAP contributors (20 features).

On a 3 MM stratified sample, retrain an XGB 20 + 1 times:

Once with the full feature set (baseline).

Twenty times, each time dropping exactly one of the selected features.

Compute

Δ
𝐺
𝑖
𝑛
𝑖
𝑗
=
𝐺
𝑖
𝑛
𝑖
baseline
−
𝐺
𝑖
𝑛
𝑖
(
−
𝑗
)
ΔGini 
j
​
 =Gini 
baseline
​
 −Gini 
(−j)
​
 
for each feature 
𝑗
j.

Across seven datasets, this produces 
7
×
10
=
70
7×10=70 ΔGini measurements (only the top-10 removals are retained to avoid symmetry with additions). These form the target for the FEAT sub-framework.

2.2.5 Cost Measurements
The cost dimension (development hours, validation effort, infrastructure hours, MLOps overhead) is captured from internal project tracking tools and does not require additional model training. Section 3.2 explains how these costs are normalised and linked to the performance deltas above.

2.2.6 Predictive-Equation Fitting
Because the algorithm and data-size experiments each supply only seven observations, their predictive equations use one explanatory factor apiece; feature impact, with 70 observations, is modelled with two factors. All relationships are estimated via ordinary least-squares linear regression; coefficients and statistical diagnostics appear in Section 4.

This controlled experimental design produces a compact yet information-rich dataset:

Sub-framework	Observations	Target Definition	Purpose
ALG	7	ΔGini between XGB and LR	Algorithm upgrade ROI
SIZE	7 curves (≈ 49 points)	Gini vs. data size	Data-scaling benefit
FEAT	70	ΔGini per feature drop	Feature engineering ROI

The setup ensures each performance delta can be attributed unambiguously to a single enhancement dimension, providing a solid empirical basis for the cost–benefit equations developed in the next section.
