2.2 Experimental Setup
To keep the white-paper concise, the entire experimental design is explained in only two sub-sections: Data Preparation (how we sample and evaluate) and Performance- & Cost-Experiments (how we create the targets that every sub-framework will predict).

2.2.1 Data Preparation
Item	Approach
Sampling unit	Stratified random rows from each local dataset (fraud Ã— 5, marketing Ã— 2).
Dev vs OOT split	Each dataset is chronologically split into a Dev window for training and a fixed Out-of-Time (OOT) window for evaluation. OOT remains identical across all experiments on the same dataset to ensure comparability.
Reference sample sizes	Algorithm and Data-Size studies: reference sample = 5 million (5 MM) rows.
Feature study: reference sample = 3 MM rows (sufficient for single-feature ablations).
Why 5 MM / 3 MM?	Benchmark tests showed that an XGB trained on 5 MM rows differs by â‰¤ 0.5 pp in Gini from training on the full dataset, but it finishes overnight on a 48-core server. 3 MM keeps the feature ablation grid tractable.

All Gini scores reported below are obtained by training on the Dev sample and evaluating on the OOT window; cross-validation is not used.

2.2.2 Performance- & Cost-Experiments
Dimension	Experimental steps (per dataset)	Output used by the framework
Algorithm upgradeÂ (ALG)	â€¢ Train Logistic Regression and XGB on the same 5 MM Dev sample.
â€¢ RecordÂ 
Î”
ğº
ğ‘–
ğ‘›
ğ‘–
ALG
=
ğº
ğ‘–
ğ‘›
ğ‘–
XGB
âˆ’
ğº
ğ‘–
ğ‘›
ğ‘–
LR
Î”Gini 
ALG
â€‹
 =Gini 
XGB
â€‹
 âˆ’Gini 
LR
â€‹
 on OOT.	7 data points (one per dataset): each 
Î”
ğº
ğ‘–
ğ‘›
ğ‘–
ALG
Î”Gini 
ALG
â€‹
  paired with a single explanatory factor.
Data-size scalingÂ (SIZE)	â€¢ From each dataset draw nested Dev subsets with sizes 
[
0.01
,
â€…â€Š
0.05
,
â€…â€Š
0.10
,
â€…â€Š
0.50
,
â€…â€Š
1
,
â€…â€Š
2
,
â€…â€Š
5
]
â€‰
MM
[0.01,0.05,0.10,0.50,1,2,5]MM.
â€¢ Train XGB on each subset, measure OOT Gini.
â€¢ Fit a three-parameter power-law curve 
ğº
ğ‘–
ğ‘›
ğ‘–
=
ğ‘“
(
rows
)
Gini=f(rows).	7 fitted curves (â‰ˆ 49 points total). Each curveâ€™s parameters become the SIZE target.
Feature impactÂ (FEAT)	â€¢ Identify top-10 and bottom-10 SHAP contributors from the model document.
â€¢ On a 3 MM Dev sample, retrain XGB 21 times (baseline + 20 single-feature drops).
â€¢ Compute 
Î”
ğº
ğ‘–
ğ‘›
ğ‘–
ğ‘—
=
ğº
ğ‘–
ğ‘›
ğ‘–
baseline
âˆ’
ğº
ğ‘–
ğ‘›
ğ‘–
(
âˆ’
ğ‘—
)
Î”Gini 
j
â€‹
 =Gini 
baseline
â€‹
 âˆ’Gini 
(âˆ’j)
â€‹
  on OOT.	70 data points (7 datasets Ã— 10 drops). Each 
Î”
ğº
ğ‘–
ğ‘›
ğ‘–
ğ‘—
Î”Gini 
j
â€‹
  is modelled with two explanatory factors.
Cost capture	Pull development hours, validation effort, and infrastructure consumption from internal tracking systems; no additional model runs are needed.	Normalised cost vectors for ALG, SIZE, and FEAT, linked later to performance deltas.

Observation inventory

Sub-framework	Performance targets	Cost vectors	Regression factors
ALG	7 Î”Gini values	7	1 factor
SIZE	7 curves (3 params Ã— 7)	7	1 factor
FEAT	70 Î”Gini values	70 (est.)	2 factors

Linear least-squares is used throughout to fit coefficients and intercepts; diagnostics are reported in Section 4. The resulting dataset is compact yet sufficient to train the three sub-frameworks and to estimate a full performanceâ€“cost trade-off equation for each enhancement dimension.
