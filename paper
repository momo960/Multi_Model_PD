3 Method

This section describes how the framework transforms raw experimental results (Â§ 2.2) into actionable estimates of performance uplift (Section 3.1), cost (Section 3.2), and finally business value (Section 3.3).
For each enhancement dimensionâ€”Algorithm, Data Size, and Featureâ€”we first identify the underlying data properties that drive performance, then design a computationally cheap proxy that can be measured before a full redevelopment cycle begins. Those proxies feed simple linear models whose coefficients are learned from the training corpus of seven business datasets.

3.1 Estimating Performance Gain

Formally, let


denote the performance gain for dataset 
ğ‘‘
d when applying enhancement 
kâˆˆ{ALG,SIZE,FEAT}.
The goal of each sub-framework is to approximate 
	â€‹

 without training the expensive target model, using only pre-computed dataset descriptors â€‹

. The descriptors differ by 
ğ‘˜
k and are detailed in the sub-sections below.

3.1.1 Algorithm-Impact Sub-Framework (ALG)
1. Drivers of Algorithmic Gain

Moving from Logistic Regression (LR) to XGBoost (XGB) adds capacity to model

non-linearity (piece-wise constant trees),

feature interactions (automatic splits), and

limited robustness to class imbalance via built-in instance weighting.
Among these, non-linearity is empirically the dominant driver Â¹; imbalance and separability mostly determine the ceiling but not the gap between LR and XGB.

2. A Practical Proxy for Non-Linearity

Directly measuring high-order interactions is computationally prohibitive. Inspired by Chen et al. (2023)Â², we instead:

Train a quick LR on a 1 MM stratified Dev subset (â‰ˆ 2 min runtime);

Evaluate Gini and macro-average F1 on the fixed OOT window;

Compute the mismatch scoreâ€‹

.

Rationale.

Gini rewards correct ranking of positives above negativesâ€”even if the classes overlap.

F1 rewards crisp separation of the two classes.
A high 
NL_proxy
NL_proxy therefore signals datasets where LR can rank reasonably well (high Gini) but fails to separate decisively (low F1) â†” strong hidden non-linearity.
Conversely, linearly separable data yield both high Gini and high F1, hence 
NL_proxy
NL_proxy â‰ˆ 1.

3. Predictive Equation

With seven datasets we fit a one-factor OLS model

 predicts the Gini uplift from LR â†’ XGB.

4. Empirical Accuracy
Domain	Mean	Absolute Error	95 % CI
Fraud (5 models)	1.36 pp	Â± 0.4 pp	
Marketing (2 models)	2.0 pp	Â± 1.1 pp	

Despite the marketing datasets exhibiting very different feature semantics and lower baseline Gini, Equation (1) generalises with â‰¤ 2 pp average errorâ€”adequate for cost-benefit triage.

5. Next Steps

Once â€‹

 is available, the framework proceeds to the Data-Size and Feature sub-frameworks (Â§ 3.1.2â€“3.1.3); together they complete the performance-uplift picture prior to adding the cost layer in Â§ 3.2.
