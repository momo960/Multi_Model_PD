To train and evaluate the proposed framework, we selected seven existing business models from internal production environments:

Five fraud detection models,

Two marketing response models.

The fraud models were chosen because they share broadly similar data structures, including comparable feature engineering approaches and target definitions, making it easier to identify systematic relationships between model characteristics, performance gains, and costs. The marketing models were included to test the generalization capability of the framework, as they operate in a different domain and have lower baseline Gini scores, representing less separable prediction problems.

Model availability was partly constrained by operational access—only these seven models were eligible for inclusion. Within this constraint, the selection was guided by the need for structural similarity in the datasets, ensuring that observed performance–cost patterns would not be dominated by unrelated structural differences.

For each selected model, we compiled relevant metadata from internal documentation, including:

Business model type (fraud / marketing),

Model ID (internal reference),

Most recent update (nature of last enhancement),

Documented performance improvement from last update,

Estimated dollar savings from last update,

Dataset size (number of records),

Number of features,

Algorithm type,

Event rate (positive class proportion),

Baseline Gini (as reported in the model documentation, used as a measure of separability).

These details are summarized in Table 1, which provides an at-a-glance overview of the experimental models and highlights their diversity in scale, complexity, and baseline performance.
