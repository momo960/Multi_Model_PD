Adding or removing a single feature changes model performance through two opposing forces:

Concept	Effect on Î”Gini	Desirable direction
Relevance â€“ how much new signal the feature contributes	â†‘â€ƒ(positive)	High
Redundancy â€“ how much of that signal is already captured by existing features	â†“â€ƒ(negative)	Low

Following the Maximum Relevance / Minimum Redundancy (mRMR) principle, we approximate

	
â€…â€Š
Î”
ğ‘ƒ
^
FEAT
â€…â€Š
=
â€…â€Š
ğœ†
0
+
ğœ†
1
â€…â€Š
Relevance
âˆ’
ğœ†
2
â€…â€Š
Redundancy
â€…â€Š
		
(3)
Î”P
FEAT
	â€‹

=Î»
0
	â€‹

+Î»
1
	â€‹

Relevanceâˆ’Î»
2
	â€‹

Redundancy
	â€‹

(3)

with two inexpensive proxies that can be computed before full retraining.

1 Relevance Proxy

For candidate feature 
ğ‘“
ğ‘—
f
j
	â€‹

:

Relevance
ğ‘—
=
Gini
(
ğ‘“
ğ‘—
,
â€…â€Š
ğ‘¦
)
Relevance
j
	â€‹

=Gini(f
j
	â€‹

,y)

i.e. the univariate Gini / AUC of 
ğ‘“
ğ‘—
f
j
	â€‹

 against the target on a 1 MM Dev subset.
A higher value means 
ğ‘“
ğ‘—
f
j
	â€‹

 can separate the classes well on its own.

2 Redundancy Proxy â€“ CorrScore

Compute MaxCorr â€“ the largest absolute Pearson correlation between 
ğ‘“
ğ‘—
f
j
	â€‹

 and any existing feature.

Identify the importance rank (MacCorrRank) of that most-correlated existing feature in the baseline XGB model (SHAP ranking; 1 = most important).

Let 
ğ‘
p be the total number of existing features.

Define

CorrScore
ğ‘—
â€…â€Š
=
â€…â€Š
âˆ£
MaxCorr
âˆ£
+
MacCorrRank
ğ‘
CorrScore
j
	â€‹

=
	â€‹

MaxCorr
	â€‹

+
p
MacCorrRank
	â€‹


Interpretation.

A high CorrScore means 
ğ‘“
ğ‘—
f
j
	â€‹

 is strongly correlated with a feature that is already important â†’ little incremental value.

A low CorrScore signals novel information.

3 Model Fitting and Validation

Dataset. 70 observations (7 datasets Ã— 10 feature drops).
Regression. OLS fit of Equation (3) with two predictors (Relevance, CorrScore).

Domain	RÂ²	Mean Abs. Error (Î”Gini pp)
Fraud (50 pts)	0.78	0.55
Marketing (20 pts)	0.64	0.83

Despite very different feature semantics in marketing data, the linear rule remains within â‰ˆ 1 ppâ€”adequate for â€œgo / no-goâ€ decisions on costly feature-engineering sprints.
